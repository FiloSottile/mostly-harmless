Events are fetched from many sources. In the past, streaming APIs; now, home
timelines for users we have tokens for; soon, user timelines and mentions; in
the future, probably lists on a dedicated account, maybe webhooks and search, as
well as crawling jobs.

Any event involving private tweets is dropped on the floor.

All events are stored as raw JSON in the Messages SQLite table with information
about their source. Some basic deduplication is done application-side based on
the JSON hash.

Note: some early events with .synthetic = 1 were originally only stored
post-processed, and events before 26347dc were re-marshalled from the Go
representation, so they lack some extra fields.

Events are processed to generate useful derived tables like Tweets and Users.
The first time an event is processed, images are fetched to the media folder.

Messages is the only authoritative table. All others can be regenerated by
rerunning the process step on all events. rescan should take no more than a few
seconds at all times.

(I recently learned this is a thing, it's now cool, and it's called event sourcing.)
